{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 43\n",
      "The topics described by their top-weighted terms:\n",
      "+-----+-----------+------------------------------------------+\n",
      "|topic|termIndices|termWeights                               |\n",
      "+-----+-----------+------------------------------------------+\n",
      "|0    |[4, 14]    |[0.0674216400984457, 0.06656800976050967] |\n",
      "|1    |[3, 1]     |[0.08640859160557939, 0.05612226121382428]|\n",
      "|2    |[2, 0]     |[0.09967193430945293, 0.07481528736292267]|\n",
      "+-----+-----------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clustering Examples\n",
    "## LDA - text-based topic clustering\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Sample data\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0,'amazon is the best they rock and is awesome and amazing'),\n",
    "    (1,'amazing amazon is so super great job amazon'),\n",
    "    (2,'amazon rocks i love their awesome products'),\n",
    "    (3,'greatness and innovative is amazon they are so incredible'),\n",
    "    (4,'walmart is stupid dumb and slow'),\n",
    "    (5,'unamazing walmart is so mediocre small and crappy'),\n",
    "    (6,'walmart sucks i hate their service'),\n",
    "    (7,'terrible service walmart ugly stores'),\n",
    "    (4,'apple great products'),\n",
    "    (5,'best apple easy technology'),\n",
    "    (6,'apple own technology'),\n",
    "    (7,'smart engineers at apple'),\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "# Vocabulary\n",
    "print('Vocabulary size: %d' % \n",
    "      sentenceDataFrame.select(explode(split('sentence',' '))).distinct().count())\n",
    "\n",
    "# Tokenize into separate words\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "\n",
    "# Vectorize into word counts per sentence \n",
    "vectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\\\n",
    "    .fit(tokenized)\n",
    "vectorized = vectorizer.transform(tokenized)\n",
    "\n",
    "# Train an LDA model - top k words, maxIter iterations .\n",
    "lda = LDA(k=3, maxIter=100)\n",
    "model = lda.fit(vectorized)\n",
    "\n",
    "# Describe topics.\n",
    "topics = model.describeTopics(2)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            sentence|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|amazing amazon is...|[-23.551052908878...|[0.99874895357899...|       0.0|\n",
      "|  0.0|amazon is the bes...|[-40.800320697211...|[0.88900837814058...|       0.0|\n",
      "|  0.0|amazon rocks i lo...|[-18.896950601626...|[0.98392308104610...|       0.0|\n",
      "|  1.0|terrible service ...|[-17.798587982976...|[0.11515651378268...|       1.0|\n",
      "|  1.0|unamazing walmart...|[-21.775337362134...|[0.98353891148092...|       0.0|\n",
      "|  1.0|walmart sucks i h...|[-16.913028428088...|[0.76343633360712...|       0.0|\n",
      "|  1.0|telus terrible se...|[-14.803740064823...|[0.01781637196109...|       1.0|\n",
      "|  0.0|ebay is the best ...|[-41.501302394572...|[0.97100314660092...|       0.0|\n",
      "|  1.0|sears is stupid d...|[-14.668494931038...|[0.15329701633718...|       1.0|\n",
      "|  1.0|unamazing sears i...|[-21.122890656098...|[0.97496286142929...|       0.0|\n",
      "|  0.0|best microsoft ea...|[-11.848728532261...|[0.50242061154462...|       0.0|\n",
      "|  0.0|microsoft own tec...|[-7.1826704004996...|[0.84392170732584...|       0.0|\n",
      "|  1.0|horrible crappy h...|[-13.953958715156...|[0.04536885643496...|       1.0|\n",
      "|  1.0|rogers crappy use...|[-20.034208931581...|[0.00787717780450...|       1.0|\n",
      "|  1.0|rogers terrible s...|[-13.482908978104...|[0.04813962032983...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "Test set accuracy = 0.8\n"
     ]
    }
   ],
   "source": [
    "## Naive Bayes Text Classification\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0,'amazon is the best they rock and is awesome and amazing'),\n",
    "    (0.0,'amazing amazon is so super great job amazon'),\n",
    "    (0.0,'amazon rocks i love their awesome products'),\n",
    "    (0.0,'greatness and innovative is amazon they are so incredible'),\n",
    "    (1.0,'walmart is stupid dumb and slow'),\n",
    "    (1.0,'unamazing walmart is so mediocre small and crappy'),\n",
    "    (1.0,'walmart sucks i hate their service'),\n",
    "    (1.0,'terrible service walmart ugly stores'),\n",
    "    (0.0,'apple great products'),\n",
    "    (0.0,'best apple easy technology'),\n",
    "    (0.0,'apple own technology'),\n",
    "    (0.0,'smart engineers at apple'),\n",
    "    (1.0,'telus terrible service'),\n",
    "    (1.0,'worst telco telus horrible'),\n",
    "    (1.0,'telus crappy useless support'),\n",
    "    (1.0,'horrible crappy hate stupid telus'),\n",
    "    (0.0,'ebay is the best they rock and is awesome and amazing'),\n",
    "    (0.0,'amazing shaw is so super great job ebat'),\n",
    "    (0.0,'ebay rocks i love their awesome products'),\n",
    "    (0.0,'greatness and innovative is ebay they are so incredible'),\n",
    "    (1.0,'sears is stupid dumb and slow'),\n",
    "    (1.0,'unamazing sears is so mediocre small and crappy'),\n",
    "    (1.0,'sears sucks i hate their service'),\n",
    "    (1.0,'terrible service sears ugly stores'),\n",
    "    (0.0,'microsoft great products'),\n",
    "    (0.0,'best microsoft easy technology'),\n",
    "    (0.0,'microsoft own technology'),\n",
    "    (0.0,'smart engineers at microsoft'),\n",
    "    (1.0,'rogers terrible service'),\n",
    "    (1.0,'worst telco rogers horrible'),\n",
    "    (1.0,'rogers crappy useless support'),\n",
    "    (1.0,'horrible crappy hate stupid rogers')\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "# Split the data into train and test\n",
    "splits = rescaledData.randomSplit([0.6, 0.4], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "model = nb.fit(train)\n",
    "\n",
    "# select example rows to display.\n",
    "predictions = model.transform(test)\n",
    "predictions.select('label','sentence','rawPrediction','probability','prediction').show()\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------+\n",
      "|label|features                 |\n",
      "+-----+-------------------------+\n",
      "|0.0  |(3,[],[])                |\n",
      "|1.0  |(3,[0,1,2],[0.1,0.1,0.1])|\n",
      "|2.0  |(3,[0,1,2],[0.2,0.2,0.2])|\n",
      "|3.0  |(3,[0,1,2],[9.0,9.0,9.0])|\n",
      "|4.0  |(3,[0,1,2],[9.1,9.1,9.1])|\n",
      "|5.0  |(3,[0,1,2],[9.2,9.2,9.2])|\n",
      "+-----+-------------------------+\n",
      "\n",
      "Within Set Sum of Squared Errors = 0.12\n",
      "Cluster Centers: \n",
      "[ 9.1  9.1  9.1]\n",
      "[ 0.1  0.1  0.1]\n"
     ]
    }
   ],
   "source": [
    "## K-Means Clustering\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"/Users/samir/Tools/spark-2.0.2-bin-hadoop2.7/data/mllib/sample_kmeans_data.txt\")\n",
    "dataset.show(truncate=False)\n",
    "\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors.\n",
    "wssse = model.computeCost(dataset)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [0.0135332792997,-0.011096050078,0.0506678894162]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [0.0376478566655,0.0210807355387,0.0403019455927]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [0.0177810560912,-0.0559235086665,-0.0178805116564]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Word2Vec\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[u'1.0', u'0.0']\n",
      "+--------------+-----+--------------------+\n",
      "|predictedLabel|label|            features|\n",
      "+--------------+-----+--------------------+\n",
      "|           0.0|  0.0|(692,[95,96,97,12...|\n",
      "|           0.0|  0.0|(692,[121,122,123...|\n",
      "|           0.0|  0.0|(692,[124,125,126...|\n",
      "|           0.0|  0.0|(692,[126,127,128...|\n",
      "|           0.0|  0.0|(692,[126,127,128...|\n",
      "|           0.0|  0.0|(692,[127,128,129...|\n",
      "|           0.0|  0.0|(692,[153,154,155...|\n",
      "|           0.0|  0.0|(692,[154,155,156...|\n",
      "|           0.0|  0.0|(692,[155,156,180...|\n",
      "|           1.0|  1.0|(692,[123,124,125...|\n",
      "|           1.0|  1.0|(692,[123,124,125...|\n",
      "|           1.0|  1.0|(692,[123,124,125...|\n",
      "|           1.0|  1.0|(692,[124,125,126...|\n",
      "|           1.0|  1.0|(692,[124,125,126...|\n",
      "|           1.0|  1.0|(692,[124,125,126...|\n",
      "|           1.0|  1.0|(692,[125,126,127...|\n",
      "|           1.0|  1.0|(692,[126,127,128...|\n",
      "|           1.0|  1.0|(692,[127,128,129...|\n",
      "|           1.0|  1.0|(692,[127,128,129...|\n",
      "|           1.0|  1.0|(692,[127,128,155...|\n",
      "+--------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test Error = 0\n",
      "RandomForestClassificationModel (uid=rfc_0060f386aff5) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "# Random Forest ML\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"/Users/samir/Tools/spark-2.0.2-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\")\n",
    "data.show()\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "print(labelIndexer.labels)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "    \n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(20)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = model.stages[2]\n",
    "print(rfModel)  # summary only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
